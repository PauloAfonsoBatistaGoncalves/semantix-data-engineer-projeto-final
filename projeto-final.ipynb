{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Enviar os dados para o hdfs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Criando diretório para armazenar os dados</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/paulo/projeto-final/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Enviando dados para o HDFS (deve ser executado diretamente no bash)</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /home/csv-files /user/paulo/projeto-final/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por\n",
    "município.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Carregando dados</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|      data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "column_list = [\n",
    "    StructField(\"regiao\", StringType()),\n",
    "    StructField(\"estado\",StringType()),\n",
    "    StructField(\"municipio\",StringType()),\n",
    "    StructField(\"coduf\",IntegerType()),\n",
    "       StructField(\"codmun\",IntegerType()),\n",
    "       StructField(\"codRegiaoSaude\",IntegerType()),\n",
    "       StructField(\"nomeRegiaoSaude\",StringType()),\n",
    "       StructField(\"data\",StringType()),\n",
    "       StructField(\"semanaEpi\",IntegerType()),\n",
    "       StructField(\"populacaoTCU2019\",IntegerType()),\n",
    "       StructField(\"casosAcumulado\",IntegerType()),\n",
    "       StructField(\"casosNovos\",IntegerType()),\n",
    "       StructField(\"obitosAcumulado\",IntegerType()),\n",
    "       StructField(\"obitosNovos\",IntegerType()),\n",
    "       StructField(\"Recuperadosnovos\",IntegerType()),\n",
    "       StructField(\"emAcompanhamentoNovos\",IntegerType()),\n",
    "       StructField(\"interior/metropolitana\",IntegerType())\n",
    "]\n",
    "\n",
    "schema = StructType(column_list)\n",
    "\n",
    "csv_files = spark.read.option(\"delimiter\", \";\").option(\"header\",\"true\").schema(schema).csv(\"/user/paulo/projeto-final/data/csv-files\");\n",
    "csv_files.show(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Salvando dados em uma tabela hive particionada por município</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files.write.partitionBy('municipio').saveAsTable('projeto_final.painel_covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Primeira Visualização</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max, count, desc, unix_timestamp,from_unixtime, col, substring, split,lit, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "painel_covid = spark.read.table('projeto_final.painel_covid')\n",
    "painel_covid = painel_covid\\\n",
    ".withColumn(\"data_timestamp\", unix_timestamp(col(\"data\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------------+\n",
      "|regiao|RecuperadosNovos|EmAcompanhamento|\n",
      "+------+----------------+----------------+\n",
      "|Brasil|        17262646|         1317658|\n",
      "+------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casos_recuperados_e_acompanhamento_view = painel_covid.filter(\"regiao='Brasil'\")\\\n",
    ".groupBy('regiao').agg(max('recuperadosnovos').alias(\"RecuperadosNovos\"),\\\n",
    "                       max('emAcompanhamentoNovos').alias('EmAcompanhamento'))\n",
    "\n",
    "casos_recuperados_e_acompanhamento_view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Segunda Visualização</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_novos = painel_covid.select('casosNovos').filter(\"regiao='Brasil'\").sort(col('data_timestamp').desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_confirmados_view = painel_covid.filter(\"regiao='Brasil'\")\\\n",
    ".groupBy('regiao').agg(max('casosacumulado').alias(\"CasosConfirmados\"))\\\n",
    ".withColumn('CasosNovos', lit(casos_novos[0].casosNovos))\n",
    "\n",
    "\n",
    "\n",
    "casos_confirmados_view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Terceira Visualização</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_novos = painel_covid.select('obitosNovos').filter(\"regiao='Brasil'\").sort(col('data_timestamp').desc()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_acumulados_view = painel_covid.filter(\"regiao='Brasil'\")\\\n",
    ".groupBy('regiao').agg(max('obitosAcumulado').alias(\"Óbitos Acumulados\")).withColumn('Casos Novos', lit(casos_novos[0].obitosNovos))\n",
    "\n",
    "\n",
    "obitos_acumulados_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_acumulados_view = painel_covid.filter(\"regiao!='Brasil'\")\\\n",
    ".groupBy('data_timestamp').agg(sum('obitosNovos').alias(\"obitos\"))\\\n",
    ".withColumn(\"data_notificacao\", from_unixtime(col(\"data_timestamp\"), \"dd-MM-yyyy\"))\\\n",
    ".sort(desc('data_timestamp'))\n",
    "\n",
    "#obitos_acumulados_view.select('data_notificacao','obitos').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.Salvar a primeira visualização como tabela Hive</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_recuperados_e_acompanhamento_view.write.saveAsTable('projeto_final.casos_recuperados')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. Salvar a segunda visualização com formato parquet e compressão snappy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_confirmados_view.write.format('parquet')\\\n",
    ".option('compression','snappy')\\\n",
    ".saveAsTable('projeto_final.casos_confirmados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. Salvar a terceira visualização em um tópico no Kafka</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topico kafka\n",
    "\n",
    "<p>kafka-topics.sh --bootstrap-server kafka:9092 --topic obitosConfirmados --create --partitions 1 --replication-factor 1</p>\n",
    "<p>kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic obitosConfirmados</p>\n",
    "<p>kafka-topics.sh --bootstrap-server kafka:9092 --topic obitosConfirmados --delete</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_acumulados_view\\\n",
    ".withColumn(\"value\", concat(col('data_notificacao').cast(StringType()),lit(','),col('obitos').cast(StringType())))\\\n",
    ".write\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    ".option(\"topic\", \"obitosConfirmados\")\\\n",
    ".option(\"checkpointLocation\",\"/user/paulo/kafka_checkpoint\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>7. Criar a visualização pelo Spark com os dados enviados para o HDFS:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintexe_view = painel_covid.filter(\"regiao != 'null'\").groupBy('regiao')\\\n",
    ".agg(max('casosAcumulado').alias(\"obitosAcumulado\"),max('obitosAcumulado').alias(\"obitosAcumulado\"))\n",
    "\n",
    "sintexe_view.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
