{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Enviar os dados para o hdfs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Criando diretório para armazenar os dados</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/paulo/projeto-final/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Enviando dados para o HDFS (deve ser executado diretamente no bash)</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /home/csv-files /user/paulo/projeto-final/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por\n",
    "município.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Carregando dados</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|      data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "column_list = [\n",
    "    StructField(\"regiao\", StringType()),\n",
    "    StructField(\"estado\",StringType()),\n",
    "    StructField(\"municipio\",StringType()),\n",
    "    StructField(\"coduf\",IntegerType()),\n",
    "       StructField(\"codmun\",IntegerType()),\n",
    "       StructField(\"codRegiaoSaude\",IntegerType()),\n",
    "       StructField(\"nomeRegiaoSaude\",StringType()),\n",
    "       StructField(\"data\",StringType()),\n",
    "       StructField(\"semanaEpi\",IntegerType()),\n",
    "       StructField(\"populacaoTCU2019\",IntegerType()),\n",
    "       StructField(\"casosAcumulado\",IntegerType()),\n",
    "       StructField(\"casosNovos\",IntegerType()),\n",
    "       StructField(\"obitosAcumulado\",IntegerType()),\n",
    "       StructField(\"obitosNovos\",IntegerType()),\n",
    "       StructField(\"Recuperadosnovos\",IntegerType()),\n",
    "       StructField(\"emAcompanhamentoNovos\",IntegerType()),\n",
    "       StructField(\"interior/metropolitana\",IntegerType())\n",
    "]\n",
    "\n",
    "schema = StructType(column_list)\n",
    "\n",
    "csv_files = spark.read.option(\"delimiter\", \";\").option(\"header\",\"true\").schema(schema).csv(\"/user/paulo/projeto-final/data/csv-files\");\n",
    "csv_files.show(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Salvando dados em uma tabela hive particionada por município</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files.write.partitionBy('municipio').saveAsTable('projeto_final.painel_covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Primeira Visualização</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max, count, desc, unix_timestamp,from_unixtime, col, substring, split,lit, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "painel_covid = spark.read.table('projeto_final.painel_covid')\n",
    "painel_covid = painel_covid\\\n",
    ".withColumn(\"data_timestamp\", unix_timestamp(col(\"data\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_recuperados_e_acompanhamento_view = painel_covid.filter(\"regiao='Brasil'\")\\\n",
    ".groupBy('regiao').agg(max('recuperadosnovos').alias(\"RecuperadosNovos\"),\\\n",
    "                       max('emAcompanhamentoNovos').alias('EmAcompanhamento'))\n",
    "\n",
    "casos_recuperados_e_acompanhamento_view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Segunda Visualização</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_novos = painel_covid.select('casosNovos').filter(\"regiao='Brasil'\").sort(col('data_timestamp').desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_confirmados_view = painel_covid.filter(\"regiao='Brasil'\")\\\n",
    ".groupBy('regiao').agg(max('casosacumulado').alias(\"CasosConfirmados\"))\\\n",
    ".withColumn('CasosNovos', lit(casos_novos[0].casosNovos))\n",
    "\n",
    "\n",
    "\n",
    "casos_confirmados_view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Terceira Visualização</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_novos = painel_covid.select('obitosNovos').filter(\"regiao='Brasil'\").sort(col('data_timestamp').desc()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_acumulados_view = painel_covid.filter(\"regiao='Brasil'\")\\\n",
    ".groupBy('regiao').agg(max('obitosAcumulado').alias(\"Óbitos Acumulados\")).withColumn('Casos Novos', lit(casos_novos[0].obitosNovos))\n",
    "\n",
    "\n",
    "obitos_acumulados_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_acumulados_view = painel_covid.filter(\"regiao!='Brasil'\")\\\n",
    ".groupBy('data_timestamp').agg(sum('obitosNovos').alias(\"obitos\"))\\\n",
    ".withColumn(\"data_notificacao\", from_unixtime(col(\"data_timestamp\"), \"dd-MM-yyyy\"))\\\n",
    ".sort(desc('data_timestamp'))\n",
    "\n",
    "#obitos_acumulados_view.select('data_notificacao','obitos').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.Salvar a primeira visualização como tabela Hive</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_recuperados_e_acompanhamento_view.write.saveAsTable('projeto_final.casos_recuperados')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. Salvar a segunda visualização com formato parquet e compressão snappy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_confirmados_view.write.format('parquet')\\\n",
    ".option('compression','snappy')\\\n",
    ".saveAsTable('projeto_final.casos_confirmados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. Salvar a terceira visualização em um tópico no Kafka</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topico kafka\n",
    "\n",
    "<p>kafka-topics.sh --bootstrap-server kafka:9092 --topic obitosConfirmados --create --partitions 1 --replication-factor 1</p>\n",
    "<p>kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic obitosConfirmados</p>\n",
    "<p>kafka-topics.sh --bootstrap-server kafka:9092 --topic obitosConfirmados --delete</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_acumulados_view\\\n",
    ".withColumn(\"value\", concat(col('data_notificacao').cast(StringType()),lit(','),col('obitos').cast(StringType())))\\\n",
    ".write\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    ".option(\"topic\", \"obitosConfirmados\")\\\n",
    ".option(\"checkpointLocation\",\"/user/paulo/kafka_checkpoint\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>7. Criar a visualização pelo Spark com os dados enviados para o HDFS:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintexe_view = painel_covid.filter(\"regiao != 'null'\").groupBy('regiao')\\\n",
    ".agg(max('casosAcumulado').alias(\"obitosAcumulado\"),max('obitosAcumulado').alias(\"obitosAcumulado\"))\n",
    "\n",
    "sintexe_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'saveToEs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6d25165d6446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobitos_acumulados_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveToEs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark/docs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'saveToEs'"
     ]
    }
   ],
   "source": [
    "obitos_acumulados_view.saveToEs(\"spark/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
